---
title: "CSML1000 Group2 Assignment 2"
output: github_document
---

Data Set Information:

The two datasets are related to red and white variants of the Portuguese "Vinho Verde" wine. For more details, consult: [Web Link] or the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).

These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.


Attribute Information:

For more information, read [Cortez et al., 2009].
Input variables (based on physicochemical tests):
1 - fixed acidity
2 - volatile acidity
3 - citric acid
4 - residual sugar
5 - chlorides
6 - free sulfur dioxide
7 - total sulfur dioxide
8 - density
9 - pH
10 - sulphates
11 - alcohol
Output variable (based on sensory data):
12 - quality (score between 0 and 10)



# Imported library
```{r}
library(corrplot)
library(cluster)
library(factoextra)
library(reshape2)
library(tidyverse)
library(RColorBrewer)
library(scales)
library(fpc)
```

# Load dataset
```{r}
raw <- read.csv('./data/winequality-red.csv', header=TRUE, sep = ";")
#add rowid
raw <- tibble::rowid_to_column(raw, "ROWID")
head(raw, 5)
summary(raw)

#numeric data, last variable is quality which is dependent variable 
data <- raw[-c(1,13)]
```

# Structure of data
```{r}
#no NAs or Nans or blanks found
str(data)
nrow(data)
```


# visualizations
```{r}

#Melt data
melt_data = melt(raw, id.vars=c("ROWID"))
#visualize spread of data
ggplot(melt_data, mapping = aes(x = value)) + geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')

boxplot(raw[,-c(1)])

#above shows that free.sulfur and total.sulfur are wildly different scales

# IMPORTANT above shows data needs scaling


```


# correlations
```{r}

#corr of data
corrmatrix <- cor(data)
corrplot(corrmatrix, method = 'number',type="upper")
#moderate corr of 0.6ish for following
#fixed acidity - citric acid
#fixed acidity - density
#free.sulfur.dioxide-total.sulfur.dioxide
#we will keep them in

#above are visualized and validated via plot
plot(data)


```


# Data preparation

```{r}


#scale data, 13th variable is quality which is a dependent variable
data_scaled <- data.frame(scale(data))

#visualize how well data was scaled
boxplot(data_scaled)
#scaling looks good
#TODO need to remove outliers

```


# determine number of Clusters
```{r}

datafornumclusters <- data_scaled

# Determine number of clusters
wss <- (nrow(datafornumclusters)-1)*sum(apply(datafornumclusters,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(datafornumclusters,
                                     centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")

#wssplot(datafornumclusters, nc=20) 

# ##### we see an elbow at 4

```


# PCA and reduction
```{r}

# ##### build pca using princomp
data_pca1 <- princomp(data_scaled)
#examine rotations
#print(data_pca)
#examine the importance of PCs
#summary(data_pca)
#inspect principal components
# loadings shows variance and and how much each variable contributes to each components
loadings(data_pca1)
# #### above shows that first 10 give us 90% of the variance
# we don't see one variable being overbearing

#plot
plot(data_pca1)
# scree plot
plot(data_pca1, type = "lines")
#biplot
biplot(data_pca1, col = c("gray", "black"))
# ###### using princomp first 10 give us 90% of the variance

# ##### build pca using prcomp
data_pca2 <- prcomp(data_scaled)
summary(data_pca2)
#above shows first 7 account for 90% variance

#plot
plot(data_pca2)
# scree plot
plot(data_pca2, type = "lines")
#biplot
biplot(data_pca2, col = c("gray", "black"))
#above shows first 7 account for 90% variance


# ###### using prcomp: first 7 give us 90% of the variance

# First 7 principal components
components <- data.frame(data_pca2$x[,1:7])
#plot(components, pch=16, col=rgb(0,0,0,0.5))

```



# Clustering
```{r}
datatocluster <- data[-c(1,13)]

k <- kmeans(datatocluster, 4, nstart=25, iter.max=10000)
#plot(data_scaled, col=k$cluster, pch=16)
#we see above 1 outlier cluster

clusplot(datatocluster, k$cluster, main='Clusters',
         color=TRUE, shade=TRUE,
         labels=2, lines=0)

# Cluster sizes
sort(table(k$cluster))

# Plot
plotcluster(  datatocluster
            , k$cluster
            , pch = k$cluster
            , clvecd=c(1,2,3,4)
            , col=c("blue","green","purple","red"))
```

# Clustering
```{r}

set.seed(12345)
#use 10 clusters to start with
fit <- kmeans(components, 15)
# Examine the fit object – produces a lot of output
fit
#these clusters account for 98.5% variability
#pairwise plot see clustering
#pairs(data, col=c(1:10)[fit$cluster])
#lets try something else
#clusplot(data, fit$cluster, color = TRUE, shade = TRUE, labels=0, lines=0)

#elbow function to find best k
rdacb.kmeans.plot <- function (data, num_clust = 15, seed = 9876) {
  set.seed(seed)
  ss <- numeric(num_clust)
  ss[1] <- (nrow(data) - 1) * sum(apply(data, 2, var))
  for (i in 2:num_clust) {
    ss[i] <- sum(kmeans(data, centers = i)$withinss)
  }
  plot(1:num_clust, ss, type = "b", pch = 18, xlab = "# Clusters", ylab = "Total within_ss across clusters")
}
#show elbow chart
rdacb.kmeans.plot(data)

# ###### Seeing an elbow at 4, will use 4 as cluster size 


```



```{r}

fit <- kmeans(data, 4)
# Examine the fit object – produces a lot of output
fit
#these clusters account for 98.5% variability
#pairwise plot see clustering
#pairs(data, col=c(1:10)[fit$cluster])
#lets try something else
#clusplot(data, fit$cluster, color = TRUE, shade = TRUE, labels=0, lines=0)


fviz_cluster(km.final, data=data)
clusplot(data, data$cluster, color=TRUE, shade = TRUE, label=2)
```

# Conclusion
  We had seen kmeans clustering of wine data and how average silhouette measure is used to identify optimal number of clusters. 