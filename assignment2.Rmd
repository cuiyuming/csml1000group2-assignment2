---
title: "CSML1000 Group2 Assignment 2"
output: github_document
---

Data Set Information:

The two datasets are related to red and white variants of the Portuguese "Vinho Verde" wine. For more details, consult: [Web Link] or the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).

These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.


Attribute Information:

For more information, read [Cortez et al., 2009].
Input variables (based on physicochemical tests):
1 - fixed acidity
2 - volatile acidity
3 - citric acid
4 - residual sugar
5 - chlorides
6 - free sulfur dioxide
7 - total sulfur dioxide
8 - density
9 - pH
10 - sulphates
11 - alcohol
Output variable (based on sensory data):
12 - quality (score between 0 and 10)



# Imported library
```{r}
library(corrplot)
library(cluster)
library(factoextra)
```

# Load dataset
```{r}
data <- read.csv('./data/winequality-red.csv', header=TRUE, sep = ";")
head(data, 5)
summary(data)
```

# Structure of data
```{r}
#no NAs or Nans or blanks found
str(data)
nrow(data)
```

# visualizations - corr matrix
```{r}
corrmatrix <- cor(data[,-12])
corrplot(corrmatrix, method = 'pie',type="upper")
corrplot(corrmatrix, method = 'number',type="upper")

```


# Data preparation
All the atrributes are of same scale except "Free. sulfur dioxide and totol sulfur.dioxide", therefore we can can ignore those two variables
for clustering and normalization is not required.

```{r}
data <- data[-c(6,7)]

```

# PCA and reduction
```{r}
#build pca
data_pca <- prcomp(data[,-10], scale = TRUE)
#examine rotations
print(data_pca)
#examine the importance of PCs
summary(data_pca)

# ###### IMPORTANT - based on proportion, first 5 give us 85% of the variance


#plot
plot(data_pca)
# scree plot
plot(data_pca, type = "lines")
#biplot
biplot(data_pca, col = c("gray", "black"))
#see computed pc values 
head(data_pca$x, 3)
#we can do predictions for new data
#predict(data_pca, newdata = â€¦)
```


# Clustring and caculate silhouette score
## Visualization of
```{r}
```



```{r}
silhouette_score <- function(k){
  km <- kmeans(data, centers = k, nstart=25)
  ss <- silhouette(km$cluster, dist(data))
  mean(ss[, 3])
}
k <- 2:10
avg_sil <- sapply(k, silhouette_score)
plot(k, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)
```

Show Optimal number of clusters
```{r}
fviz_nbclust(data, kmeans, method='silhouette')
```


# Actual clustring
```{r}
km.final <- kmeans(data, 2)

## Total Within cluster sum of square
km.final$tot.withinss

## Cluster sizes
km.final$size
data$cluster <- km.final$cluster
head(data, 6)
```
```{r}
corrmatrix <- cor(data)
corrplot(corrmatrix, method = 'pie')
```


```{r}
fviz_cluster(km.final, data=data)
clusplot(data, data$cluster, color=TRUE, shade = TRUE, label=2)
```

# Conclusion
  We had seen kmeans clustering of wine data and how average silhouette measure is used to identify optimal number of clusters. 