---
title: "CSML1000 Group2 Assignment 2 Due this Sunday"
output: github_document
Author: Yuming, Pratik, Madan, Rajiv, Konstantin
Date: October 30, 2019
---

Data Set Information:

The two datasets are related to red and white variants of the Portuguese "Vinho Verde" wine. For more details, consult: [Web Link] or the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).

These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.


Attribute Information:

For more information, read [Cortez et al., 2009].
Input variables (based on physicochemical tests):
1 - fixed acidity
2 - volatile acidity
3 - citric acid
4 - residual sugar
5 - chlorides
6 - free sulfur dioxide
7 - total sulfur dioxide
8 - density
9 - pH
10 - sulphates
11 - alcohol
Output variable (based on sensory data):
12 - quality (score between 0 and 10)



# Imported library
```{r}
library(corrplot)
library(cluster)
library(factoextra)
library(reshape2)
library(tidyverse)
library(RColorBrewer)
library(scales)
library(fpc)
library(EnvStats)
library(NbClust)
```

# Load dataset
```{r}
raw <- read.csv('./data/winequality-red.csv', header=TRUE, sep = ";")
#add rowid
raw <- tibble::rowid_to_column(raw, "ROWID")
head(raw, 5)
summary(raw)

#numeric data, last variable is quality which is dependent variable 
data <- raw[-c(1,13)]
```

# Structure of data
```{r}
#no NAs or Nans or blanks found
str(data)
nrow(data)
```


# visualizations
```{r}

#Melt data
melt_data = melt(raw, id.vars=c("ROWID"))
#visualize spread of data
ggplot(melt_data, mapping = aes(x = value)) + geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')

boxplot(raw[,-c(1)])

# above shows data needs scaling and has outliers

#checking outliers

#3 outlier
rosnerTest(data$volatile.acidity, k = 4, warn = F)
#0 outliers
rosnerTest(data$citric.acid, k = 4, warn = F)
#4 outliers
rosnerTest(data$residual.sugar, k = 4, warn = F)
#4 outliers
rosnerTest(data$chlorides, k = 4, warn = F)
#4 outliers
rosnerTest(data$free.sulfur.dioxide, k = 4, warn = F)
#0 outliers
rosnerTest(data$density, k = 4, warn = F)
#2 outliers
rosnerTest(data$pH, k = 4, warn = F)
#4 outliers
rosnerTest(data$sulphates, k = 4, warn = F)
#1 outlier
rosnerTest(data$alcohol, k = 4, warn = F)

```


# correlations
```{r}

#corr of data
corrmatrix <- cor(data)
corrplot(corrmatrix, method = 'number',type="upper")
#moderate corr of 0.6ish for following
#fixed acidity - citric acid
#fixed acidity - density
#free.sulfur.dioxide-total.sulfur.dioxide
#we will remove fixed acidity and total.sulfur
data <- data[-c(1,7)]


#above are visualized and validated via plot
plot(data)


```


# Data preparation

```{r}
#replace outliers with 5th and 95th percentile values
#remember An outlier is not any point over the 95th percentile 
#or below the 5th percentile. Instead, an outlier is considered so 
#if it is below the first quartile – 1.5·IQR or above third quartile + 1.5·IQR.

capOutlier <- function(x){
   qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
   caps <- quantile(x, probs=c(.05, .95), na.rm = T)
   H <- 1.5 * IQR(x, na.rm = T)
   x[x < (qnt[1] - H)] <- caps[1]
   x[x > (qnt[2] + H)] <- caps[2]
   return(x)
}
data$volatile.acidity=capOutlier(data$volatile.acidity)
data$residual.sugar=capOutlier(data$residual.sugar)
data$chlorides=capOutlier(data$chlorides)
data$free.sulfur.dioxide=capOutlier(data$free.sulfur.dioxide)
data$density=capOutlier(data$density)
data$pH=capOutlier(data$pH)
data$sulphates=capOutlier(data$sulphates)
data$alcohol=capOutlier(data$alcohol)

#scale data
data_scaled <- data.frame(scale(data))

#visualize how well data was scaled
boxplot(data_scaled)
#scaling looks good


```



# PCA and reduction
```{r}

# ##### build pca using princomp
data_pca1 <- princomp(data_scaled)
#examine rotations
#print(data_pca)
#examine the importance of PCs
#summary(data_pca)
#inspect principal components
# loadings shows variance and and how much each variable contributes to each components
loadings(data_pca1)
# #### above shows that first 8 give us 89% of the variance
# we don't see one variable being overbearing

#plot
plot(data_pca1)
# scree plot
plot(data_pca1, type = "lines")
#biplot
biplot(data_pca1, col = c("gray", "black"))
# ###### using princomp first 10 give us 90% of the variance

# ##### build pca using prcomp
data_pca2 <- prcomp(data_scaled)
summary(data_pca2)
#above shows first 6 account for 88% variance

#plot
plot(data_pca2)
# scree plot
plot(data_pca2, type = "lines")
#biplot
biplot(data_pca2, col = c("gray", "black"))
#above shows first 7 account for 90% variance

# ####PCA didnt really didnt help at all because of the number of components
# We were hoping for a reduction down to 3 - 5 components
#sigh..
#stick with scaled data


```


# determine number of Clusters

```{r}

datatocluster <- data_scaled

# Silhouette method recommends 2
fviz_nbclust(datatocluster, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

# Elbow method seems to show elbow at 4 or 5 
fviz_nbclust(datatocluster, kmeans, method = "wss")+
  labs(subtitle = "Elbow method")

#gap stat takes some time, uncomment to run it
# Gap statistic recommends 1
set.seed(123)
#fviz_nbclust(datatocluster, kmeans, nstart = 25,  method = "gap_stat", nboot = 50) + labs(subtitle = "Gap statistic method")

#euclidean takes some time, uncomment to run it
#lets also try nbclust using hierarchical clustering
#recommends 4
#NbClust(data = datatocluster, diss = NULL, distance = "euclidean",min.nc = 2, max.nc = 15, method = "ward.D")

# ##### optimal clusters is 4
k=4;

```

# #############################
# Clustering using kmeans

```{r}

fit <- kmeans(datatocluster, k, nstart=25, iter.max=1000)

fit


# Cluster sizes
sort(table(fit$cluster))

```


# visualize and analyze clusters generated by kmeans
```{r}

# ####PCA didnt really didnt help at all because of the number of components
# We were hoping for a reduction down to 3 - 5 components
#sigh..
#stick with scaled data

datatovisualize1 <- data_scaled


#clusplot below, but is useless as we performed analysis in 9 dimensions
#clusplot below uses first 2 dimensions which covers 47% variability
#too much overlap in 2d
#should really plot pairs or each variable against cluster
clusplot(datatovisualize1, fit$cluster,cex=1,xlab=colnames(datatovisualize1)[1],ylab=colnames(datatovisualize1)[2],col.p=fit$cluster,lines=0,labels=1)

#pairwise plot
#pairwise keeps running and crashes
#uncomment if need to run
#pairs(datatovisualize1, col=c("red","blue","green","yellow")[fit$cluster])

#put quality back in data
datatovisualize1$quality <- raw$quality

old.par <- par(mar = c(0, 0, 0, 0))
par(mfrow=c(5,2))


for(i in 1:10){
  boxplot(datatovisualize1[,i] ~ fit$cluster,
        xlab='Cluster Number', ylab=colnames(datatovisualize1)[i],
        main=paste('Clusters of ', as.character(colnames(datatovisualize1)[i])))
}
#reset graphics
par(old.par)

#cluster 3 is higher quality than others
#why is that?
# cluster 3: alcohol, pH are higher
# cluster 3: sugar, chlorides, density are lower

#put cluster back into raw data so we can save raw data and show in shiny app
raw$cluster_kmeans=fit$cluster

```


# #############################
# Clustering using Hierarchical Clustering

```{r}
#we implement a Ward’s hierarchical clustering procedure:
#distance matrix
d <- dist(data_scaled,method = "euclidean") 
#clustering
h_clust <- hclust(d, method = "ward.D2") 


#display dendrogram
plot(h_clust)
#extract clusters
groups <- cutree(h_clust,k=4)
groups


```

# visualize and analyze clusters generated by hierarchical clustering
```{r}
# ####PCA didnt really didnt help at all because of the number of components
# We were hoping for a reduction down to 3 - 5 components
#sigh..
#stick with scaled data

datatovisualize2 <- data_scaled


# 2D representation of the Segmentation:
clusplot(datatovisualize2, groups, main='Clusters')

#pairwise plot
#pairwise keeps running and crashes
#uncomment if need to run
#pairs(datatovisualize2, col=c("red","blue","green","yellow")[groups])

#put quality back in data
datatovisualize2$quality <- raw$quality

par(mfrow=c(5,2))


for(i in 1:10){
  boxplot(datatovisualize2[,i] ~ groups,
        xlab='Cluster Number', ylab=colnames(datatovisualize2)[i],
        main=paste('Clusters of ', as.character(colnames(datatovisualize2)[i])))
}
#reset graphics
par(old.par)

#cluster 4 is higher quality than others
#why is that?
# cluster 4: alcohol is higher
# cluster 4: chlorides, density are lower

#put cluster back into raw data so we can save raw data and show in shiny app
raw$cluster_hclust=groups

#TODO save data to file to load in shiny

```


# #############################
# Clustering using another method

```{r}


```

# visualize and analyze clusters generated by method
```{r}


```


# Conclusion
  We had seen kmeans clustering of wine data and how average silhouette measure is used to identify optimal number of clusters. 