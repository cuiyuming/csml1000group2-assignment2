---
title: "CSML1000 Group2 Assignment 2"
output: github_document
---

Data Set Information:

The two datasets are related to red and white variants of the Portuguese "Vinho Verde" wine. For more details, consult: [Web Link] or the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).

These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.


Attribute Information:

For more information, read [Cortez et al., 2009].
Input variables (based on physicochemical tests):
1 - fixed acidity
2 - volatile acidity
3 - citric acid
4 - residual sugar
5 - chlorides
6 - free sulfur dioxide
7 - total sulfur dioxide
8 - density
9 - pH
10 - sulphates
11 - alcohol
Output variable (based on sensory data):
12 - quality (score between 0 and 10)



# Imported library
```{r}
library(corrplot)
library(cluster)
library(factoextra)
library(reshape2)
library(tidyverse)
library(RColorBrewer)
library(scales)
library(fpc)
```

# Load dataset
```{r}
raw <- read.csv('./data/winequality-red.csv', header=TRUE, sep = ";")
#add rowid
raw <- tibble::rowid_to_column(raw, "ROWID")
head(raw, 5)
summary(raw)

#numeric data, last variable is quality which is dependent variable 
data <- raw[-c(1,13)]
```

# Structure of data
```{r}
#no NAs or Nans or blanks found
str(data)
nrow(data)
```


# visualizations
```{r}

#Melt data
melt_data = melt(raw, id.vars=c("ROWID"))
#visualize spread of data
ggplot(melt_data, mapping = aes(x = value)) + geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')

boxplot(raw[,-c(1)])

#above shows that total.sulfur has outliers

# IMPORTANT above shows data needs scaling


```


# correlations
```{r}

#corr of data
corrmatrix <- cor(data)
corrplot(corrmatrix, method = 'number',type="upper")
#moderate corr of 0.6ish for following
#fixed acidity - citric acid
#fixed acidity - density
#free.sulfur.dioxide-total.sulfur.dioxide
#we will remove fixed acidity and total.sulfur later
data <- data[-c(1,7)]


#above are visualized and validated via plot
plot(data)


```


# Data preparation

```{r}


#scale data
data_scaled <- data.frame(scale(data))

#visualize how well data was scaled
boxplot(data_scaled)
#scaling looks good

#remove outliers

#For missing values that lie outside the 1.5 * IQR limits
#we could cap it by replacing those observations outside the lower limit
#with the value of 5th %ile and those that lie above the upper limit,
#with the value of 95th %ile. Below is a sample code that achieves this.

#x <- data$volatile.acidity
#qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
#caps <- quantile(x, probs=c(.05, .95), na.rm = T)
#H <- 1.5 * IQR(x, na.rm = T)
#x[x < (qnt[1] - H)]  (qnt[2] + H)] <- caps[2]

```


# determine number of Clusters
```{r}


rdacb.kmeans.plot <- function (datafornumclusters, num_clust = 15, seed = 9876) {
  set.seed(seed)
  ss <- numeric(num_clust)
  ss[1] <- (nrow(datafornumclusters) - 1) * sum(apply(datafornumclusters, 2, var))
  for (i in 2:num_clust) {
    ss[i] <- sum(kmeans(datafornumclusters, centers = i)$withinss)
  }
  plot(1:num_clust, ss, type = "b", pch = 18, xlab = "# Clusters", ylab = "Total within_ss across clusters")
}

rdacb.kmeans.plot(data_scaled)

# ##### we see an elbow at 12

```


# PCA and reduction
```{r}

# ##### build pca using princomp
data_pca1 <- princomp(data_scaled)
#examine rotations
#print(data_pca)
#examine the importance of PCs
#summary(data_pca)
#inspect principal components
# loadings shows variance and and how much each variable contributes to each components
loadings(data_pca1)
# #### above shows that first 8 give us 89% of the variance
# we don't see one variable being overbearing

#plot
plot(data_pca1)
# scree plot
plot(data_pca1, type = "lines")
#biplot
biplot(data_pca1, col = c("gray", "black"))
# ###### using princomp first 10 give us 90% of the variance

# ##### build pca using prcomp
data_pca2 <- prcomp(data_scaled)
summary(data_pca2)
#above shows first 6 account for 88% variance

#plot
plot(data_pca2)
# scree plot
plot(data_pca2, type = "lines")
#biplot
biplot(data_pca2, col = c("gray", "black"))
#above shows first 7 account for 90% variance

# ###### using prcomp: first 7 give us 90% of the variance

# First 7 principal components
components <- data.frame(data_pca2$x[,1:7])
#plot(components, pch=16, col=rgb(0,0,0,0.5))

```



# Clustering
```{r}
datatocluster <- components

k <- kmeans(datatocluster, 12, nstart=25, iter.max=10000)

k

#pairwise plot
pairs(datatocluster, col=c(1:5)[k$cluster])

# Plot
plotcluster(  datatocluster
            , k$cluster
            , pch = k$cluster
            )

#clusplot(datatocluster, k$cluster, main='Clusters',
#         color=TRUE, shade=TRUE,
#         labels=2, lines=0)

# Cluster sizes
sort(table(k$cluster))


```




# Conclusion
  We had seen kmeans clustering of wine data and how average silhouette measure is used to identify optimal number of clusters. 