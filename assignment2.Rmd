---
title: "CSML1000 Group2 Assignment 2"
output: github_document
---

Data Set Information:

The two datasets are related to red and white variants of the Portuguese "Vinho Verde" wine. For more details, consult: [Web Link] or the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).

These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.


Attribute Information:

For more information, read [Cortez et al., 2009].
Input variables (based on physicochemical tests):
1 - fixed acidity
2 - volatile acidity
3 - citric acid
4 - residual sugar
5 - chlorides
6 - free sulfur dioxide
7 - total sulfur dioxide
8 - density
9 - pH
10 - sulphates
11 - alcohol
Output variable (based on sensory data):
12 - quality (score between 0 and 10)



# Imported library
```{r}
library(corrplot)
library(cluster)
library(factoextra)
library(reshape2)
library(tidyverse)
library(RColorBrewer)
library(scales)
library(fpc)
library(EnvStats)
library(NbClust)
```

# Load dataset
```{r}
raw <- read.csv('./data/winequality-red.csv', header=TRUE, sep = ";")
#add rowid
raw <- tibble::rowid_to_column(raw, "ROWID")
head(raw, 5)
summary(raw)

#numeric data, last variable is quality which is dependent variable 
data <- raw[-c(1,13)]
```

# Structure of data
```{r}
#no NAs or Nans or blanks found
str(data)
nrow(data)
```


# visualizations
```{r}

#Melt data
melt_data = melt(raw, id.vars=c("ROWID"))
#visualize spread of data
ggplot(melt_data, mapping = aes(x = value)) + geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')

boxplot(raw[,-c(1)])

# above shows data needs scaling and has outliers

#checking outliers

#3 outlier
rosnerTest(data$volatile.acidity, k = 4, warn = F)
#0 outliers
rosnerTest(data$citric.acid, k = 4, warn = F)
#4 outliers
rosnerTest(data$residual.sugar, k = 4, warn = F)
#4 outliers
rosnerTest(data$chlorides, k = 4, warn = F)
#4 outliers
rosnerTest(data$free.sulfur.dioxide, k = 4, warn = F)
#0 outliers
rosnerTest(data$density, k = 4, warn = F)
#2 outliers
rosnerTest(data$pH, k = 4, warn = F)
#4 outliers
rosnerTest(data$sulphates, k = 4, warn = F)
#1 outlier
rosnerTest(data$alcohol, k = 4, warn = F)

```


# correlations
```{r}

#corr of data
corrmatrix <- cor(data)
corrplot(corrmatrix, method = 'number',type="upper")
#moderate corr of 0.6ish for following
#fixed acidity - citric acid
#fixed acidity - density
#free.sulfur.dioxide-total.sulfur.dioxide
#we will remove fixed acidity and total.sulfur
data <- data[-c(1,7)]


#above are visualized and validated via plot
plot(data)


```


# Data preparation

```{r}
#replace outliers with 5th and 95th percentile values
#remember An outlier is not any point over the 95th percentile 
#or below the 5th percentile. Instead, an outlier is considered so 
#if it is below the first quartile – 1.5·IQR or above third quartile + 1.5·IQR.

capOutlier <- function(x){
   qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
   caps <- quantile(x, probs=c(.05, .95), na.rm = T)
   H <- 1.5 * IQR(x, na.rm = T)
   x[x < (qnt[1] - H)] <- caps[1]
   x[x > (qnt[2] + H)] <- caps[2]
   return(x)
}
data$volatile.acidity=capOutlier(data$volatile.acidity)
data$residual.sugar=capOutlier(data$residual.sugar)
data$chlorides=capOutlier(data$chlorides)
data$free.sulfur.dioxide=capOutlier(data$free.sulfur.dioxide)
data$density=capOutlier(data$density)
data$pH=capOutlier(data$pH)
data$sulphates=capOutlier(data$sulphates)
data$alcohol=capOutlier(data$alcohol)

#scale data
data_scaled <- data.frame(scale(data))

#visualize how well data was scaled
boxplot(data_scaled)
#scaling looks good


```



# PCA and reduction
```{r}

# ##### build pca using princomp
data_pca1 <- princomp(data_scaled)
#examine rotations
#print(data_pca)
#examine the importance of PCs
#summary(data_pca)
#inspect principal components
# loadings shows variance and and how much each variable contributes to each components
loadings(data_pca1)
# #### above shows that first 8 give us 89% of the variance
# we don't see one variable being overbearing

#plot
plot(data_pca1)
# scree plot
plot(data_pca1, type = "lines")
#biplot
biplot(data_pca1, col = c("gray", "black"))
# ###### using princomp first 10 give us 90% of the variance

# ##### build pca using prcomp
data_pca2 <- prcomp(data_scaled)
summary(data_pca2)
#above shows first 6 account for 88% variance

#plot
plot(data_pca2)
# scree plot
plot(data_pca2, type = "lines")
#biplot
biplot(data_pca2, col = c("gray", "black"))
#above shows first 7 account for 90% variance

# ###### using prcomp: first 7 give us 90% of the variance

# First 7 principal components
data_pca <- data.frame(data_pca2$x[,1:7])
#plot(components, pch=16, col=rgb(0,0,0,0.5))

```


# determine number of Clusters

```{r}

datatocluster <- data_pca

# Silhouette method recommends 2
fviz_nbclust(datatocluster, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

# Elbow method seems to show elbow at 4 or 5 
fviz_nbclust(datatocluster, kmeans, method = "wss")+
  labs(subtitle = "Elbow method")

# Gap statistic recommends 1
set.seed(123)
fviz_nbclust(datatocluster, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
  labs(subtitle = "Gap statistic method")

#lets also try nbclust using hierarchical clustering
#recommends 4
NbClust(data = datatocluster, diss = NULL, distance = "euclidean",
        min.nc = 2, max.nc = 15, method = "ward.D")

# ##### optimal clusters is 4
k=4;

```

# #############################
# Clustering using kmeans

```{r}

fit <- kmeans(datatocluster, k, nstart=25, iter.max=1000)

fit

#merge back into orig dataset
raw$cluster_kmeans <- fit$cluster 
data_scaled$cluster_kmeans <- fit$cluster
#also put back in quality
data_scaled$quality <- raw$quality


# Cluster sizes
sort(table(fit$cluster))

```


# visualize and analyze clusters generated by kmeans
```{r}
datatovisualize <- data_scaled


#df = melt(data_scaled, id.vars=c("quality"),variable.name = 'chemical')
# or plot on different plots
#ggplot(df, aes(quality,value)) + geom_point(alpha=0.1,aes(color=cluster_kmeans)) + facet_grid(chemical ~ .)


#clusplot below, but is useless as we performed analysis in 7 dimensions (based on PCA)
#clusplot below uses first 2 dimensions which contribute to 29% variability
#too much overlap in 2d
#should really plot pairs or each variable against cluster
clusplot(datatovisualize, fit$cluster, main='Clusters')


# Compare volatile.acidity by cluster in boxplot
boxplot(datatovisualize$volatile.acidity ~ fit$cluster,
        xlab='Cluster', ylab='Volatile.Acidity',
        main='Volatile Acidity by Cluster')

# Compare citric.acid by cluster in boxplot
boxplot(datatovisualize$citric.acid ~ fit$cluster,
        xlab='Cluster', ylab='citric.acid',
        main='Citric Acid by Cluster')

# Compare quality by cluster in boxplot
boxplot(raw$quality ~ fit$cluster,
        xlab='Cluster', ylab='Wine Quality',
        main='Wine Quality by Cluster')

#WOW- clusters 1,3,4 are Quality=5 or 6
# and cluster 2 is quality 6 or 7 

#Is there anything that makes cluster 2 stand out?
#lets explore using pairwise plot
#pairwise plot
#cluster 2 is ....
#pairs(datatovisualize, col=c("red","blue","green","yellow")[fit$cluster])



```


# #############################
# Clustering using Hierarchical Clustering

```{r}
#we implement a Ward’s hierarchical clustering procedure:
#distance matrix
d <- dist(data_scaled,method = "euclidean") 
#clustering
h_clust <- hclust(d, method = "ward.D2") 
#display dendrogram
plot(h_clust)
#extract clusters
groups <- cutree(h_clust,k=4)
groups

```

# visualize and analyze clusters generated by hierarchical clustering
```{r}
# 2D representation of the Segmentation:
clusplot(data_scaled, groups, color=TRUE, shade=TRUE,
         labels=2, lines=0, main= 'Wine Categories')

```


# #############################
# Clustering using another method

```{r}


```

# visualize and analyze clusters generated by method
```{r}


```


# Conclusion
  We had seen kmeans clustering of wine data and how average silhouette measure is used to identify optimal number of clusters. 